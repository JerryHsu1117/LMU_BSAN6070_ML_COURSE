{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0725fd57",
      "metadata": {},
      "source": [
        "# CA02 \n",
        "\n",
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
        "\n",
        "Jerry and Nicholson"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9ffb66",
      "metadata": {},
      "source": [
        "# Assignment Overview:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1650cd6c",
      "metadata": {},
      "source": [
        "This assignment is to build a machine learning model that can classify emails as spam or not spam using the Naive Bayes algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "Goal:\n",
        "\n",
        "Build a model that predicts whether an email is Spam (1) or Not Spam (0).\n",
        "\n",
        "---\n",
        "\n",
        "Key challenge: \n",
        "\n",
        "This dataset is not a CSV table. Each email is stored as a separate .txt file inside \n",
        "folders:\n",
        "\n",
        "train-mails/ (used to learn patterns)\n",
        "\n",
        "test-mails/ (used to evaluate performance)\n",
        "\n",
        "---\n",
        "\n",
        "Cell's workflow:\n",
        "\n",
        "Read training emails and build a dictionary of common words\n",
        "\n",
        "Convert each email into a numeric feature vector (word counts)\n",
        "\n",
        "Assign labels using filename rule (spmsg* = spam)\n",
        "\n",
        "Train Naive Bayes model\n",
        "\n",
        "Predict test labels and compute accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee960bd",
      "metadata": {},
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47b5690",
      "metadata": {},
      "source": [
        "# Imports Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "edd4f298",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import Counter # count word frequency efficiently\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score # evaluate prediction accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbf933f",
      "metadata": {},
      "source": [
        "# Part1 Build the Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9c062e",
      "metadata": {},
      "source": [
        "Purpose:\n",
        "\n",
        "Decide which words become our features by counting words across ALL emails (train + test) and keeping the top 3000.\n",
        "\n",
        "Breakdown of the code logic:\n",
        "\n",
        "1. Collect all email file paths from both folders (train + test).\n",
        "2. Open each email and read the full text.\n",
        "3. Split the text into words and collect all words into one big list.\n",
        "4. Use `Counter()` to count how often each word appears.\n",
        "5. Remove noisy tokens:\n",
        "   - non-alphabet (numbers, symbols, punctuation)\n",
        "   - one-letter words\n",
        "6. Keep only the top `top_k` most frequent words as the final vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0e4b41db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def dictionary(train_dir, test_dir, top_k=3000):\n",
        "    all_tokens = []\n",
        "\n",
        "    # collect all files from both folders\n",
        "    all_files = []\n",
        "    for folder in [train_dir, test_dir]:\n",
        "        for fname in os.listdir(folder):\n",
        "            fpath = os.path.join(folder, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                all_files.append(fpath)\n",
        "\n",
        "    # read each email and collect tokens\n",
        "    for fpath in all_files:\n",
        "        with open(fpath, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
        "            text = f.read()\n",
        "            words = text.split()\n",
        "            all_tokens.extend(words)\n",
        "\n",
        "    # count frequency of each token\n",
        "    freq = Counter(all_tokens)\n",
        "\n",
        "    # remove noisy tokens: non-alpha and one-letter\n",
        "    for token in list(freq.keys()):\n",
        "        if (not token.isalpha()) or (len(token) == 1):\n",
        "            del freq[token]\n",
        "\n",
        "    # keep top K frequent words\n",
        "    vocab = freq.most_common(top_k)\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31dadea2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3d491d70",
      "metadata": {},
      "source": [
        "## Breakdown of the code logic:\n",
        "\n",
        "1. Collect all email file paths from both folders (train + test).\n",
        "2. Open each email and read the full text.\n",
        "3. Split the text into words and collect all words into one big list.\n",
        "4. Use `Counter()` to count how often each word appears.\n",
        "5. Remove noisy tokens:\n",
        "   - non-alphabet (numbers, symbols, punctuation)\n",
        "   - one-letter words\n",
        "6. Keep only the top `top_k` most frequent words as the final vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb205ee",
      "metadata": {},
      "source": [
        "# Part2 Convert Emails into Features and Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac88915",
      "metadata": {},
      "source": [
        "Purpose:\n",
        "\n",
        "This part is to transform each email from text file into a numerical feature vector based on word frequencies and assign a spam or non-spam label using the file naming rule, so the data becomes usable for Naive Bayes classification.\n",
        "\n",
        "The goal to create two outputs, feature matrix and label array, these two outputs are the input required to train and test the Naive Bayes model.\n",
        "\n",
        "Breakdown of the code logic:\n",
        "\n",
        "1. List all email files in the given folder.\n",
        "2. Create an empty feature matrix `X`:\n",
        "   - rows = number of emails\n",
        "   - columns = 3000 words in vocabulary\n",
        "3. Create an empty label array `y`.\n",
        "4. Create a fast lookup map: word → column index.\n",
        "5. For each email:\n",
        "   - read full email text\n",
        "   - split into words\n",
        "   - count words using `Counter`\n",
        "   - fill the correct columns in the feature matrix\n",
        "6. Create label from filename:\n",
        "   - starts with `spmsg` → spam (1)\n",
        "   - otherwise → not spam (0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fc2fda2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Convert emails into feature matrix + labels\n",
        "# -------------------------\n",
        "def featurize_folder(folder_dir, vocab, top_k=3000):\n",
        "    files = [\n",
        "        os.path.join(folder_dir, f)\n",
        "        for f in os.listdir(folder_dir)\n",
        "        if os.path.isfile(os.path.join(folder_dir, f))\n",
        "    ]\n",
        "\n",
        "    X = np.zeros((len(files), top_k))\n",
        "    y = np.zeros(len(files))\n",
        "\n",
        "    # fast lookup: word -> column index\n",
        "    word_to_col = {w: i for i, (w, _) in enumerate(vocab)}\n",
        "\n",
        "    for row_id, fpath in enumerate(files):\n",
        "        # read full email text\n",
        "        with open(fpath, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
        "            text = f.read()\n",
        "            words = text.split()\n",
        "\n",
        "        # count words once per email\n",
        "        counts = Counter(words)\n",
        "\n",
        "        # fill vector\n",
        "        for word, cnt in counts.items():\n",
        "            if word in word_to_col:\n",
        "                X[row_id, word_to_col[word]] = cnt\n",
        "\n",
        "        # label from filename\n",
        "        filename = os.path.basename(fpath)\n",
        "        y[row_id] = 1 if filename.startswith(\"spmsg\") else 0\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51ca029",
      "metadata": {},
      "source": [
        "# Part3 Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5070e7fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter the paths of the training and testing folders\n",
        "# Update these if your data is stored elsewhere\n",
        "TRAIN_FOLDER = \"./train-mails\"\n",
        "TEST_FOLDER  = \"./test-mails\"\n",
        "\n",
        "MAX_FEATURES = 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1995ee8f",
      "metadata": {},
      "source": [
        "# Part4 Run Feature Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9075848",
      "metadata": {},
      "source": [
        "Purpose:\n",
        "\n",
        "Build the dictionary first, then convert training and testing emails into numeric features.\n",
        "\n",
        "Breakdown of the code logic:\n",
        "\n",
        "1. Build vocabulary using train + test emails (top 3000 words).\n",
        "2. Convert training emails into `(X_train, y_train)`.\n",
        "3. Convert testing emails into `(X_test, y_test)`.\n",
        "4. After this step, we have numbers ready for machine learning training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c726beba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building dictionary using TRAIN + TEST emails ...\n",
            "Featurizing training emails ...\n",
            "Featurizing testing emails ...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run pipeline\n",
        "# -------------------------\n",
        "print(\"Building dictionary using TRAIN + TEST emails ...\")\n",
        "vocabulary = dictionary(TRAIN_FOLDER, TEST_FOLDER, top_k=MAX_FEATURES)\n",
        "\n",
        "print(\"Featurizing training emails ...\")\n",
        "X_train, y_train = featurize_folder(TRAIN_FOLDER, vocabulary, top_k=MAX_FEATURES)\n",
        "\n",
        "print(\"Featurizing testing emails ...\")\n",
        "X_test, y_test = featurize_folder(TEST_FOLDER, vocabulary, top_k=MAX_FEATURES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798a907c",
      "metadata": {},
      "source": [
        "# Part5 Train Naive Bayes and Evaluate Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd88f2f9",
      "metadata": {},
      "source": [
        "Purpose:\n",
        "\n",
        "Train the Naive Bayes model using training features, then test it on test features and report accuracy.\n",
        "\n",
        "Breakdown of the code logic:\n",
        "\n",
        "1. Create a `MultinomialNB` model (good for word counts).\n",
        "2. Train the model using `X_train` and `y_train`.\n",
        "3. Predict labels for test data `X_test`.\n",
        "4. Compare predictions with `y_test`.\n",
        "5. Print accuracy score (how many emails are classified correctly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "96527a4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model using Multinomial Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Train + Test Naive Bayes\n",
        "# -------------------------\n",
        "print(\"Training Model using Multinomial Naive Bayes algorithm .....\")\n",
        "nb_model = MultinomialNB(alpha=1.0)  # smoothing helps unseen word cases\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training completed\")\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print(accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c638a7b",
      "metadata": {},
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
